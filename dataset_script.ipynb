{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFYLk/nyq+NvJ5aqT8wt0F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yacine715/GITHUB_TOKEN/blob/main/dataset_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxq66YMgOUUO"
      },
      "outputs": [],
      "source": [
        "import token\n",
        "\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "import shutil\n",
        "from datetime import timezone\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Use environment variables for sensitive information\n",
        "github_token = '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'\n",
        "\n",
        "output_csv = 'builds-final-10.csv'\n",
        "\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "def get_request(url, token):\n",
        "    headers = {'Authorization': f'token {token}'}\n",
        "    attempt = 0\n",
        "    while attempt < 5:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        elif response.status_code == 403 and 'X-RateLimit-Reset' in response.headers:\n",
        "            reset_time = datetime.fromtimestamp(int(response.headers['X-RateLimit-Reset']), timezone.utc)\n",
        "            sleep_time = (reset_time - datetime.now(timezone.utc)).total_seconds() + 10\n",
        "            logging.error(f\"Rate limit exceeded, sleeping for {sleep_time} seconds. URL: {url}\")\n",
        "            time.sleep(sleep_time)\n",
        "        else:\n",
        "            logging.error(f\"Failed to fetch data, status code: {response.status_code}, URL: {url}, Response: {response.text}\")\n",
        "            time.sleep(math.pow(2, attempt) * 10)  # Exponential backoff\n",
        "        attempt += 1\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def chercher_projets_avec_build_yml(token, query='filename:build.yml path:.github/workflows', per_page=100):\n",
        "    \"\"\"\n",
        "    Recherche des projets GitHub utilisant un fichier build.yml.\n",
        "    \"\"\"\n",
        "    url = \"https://api.github.com/search/code\"\n",
        "    headers = {'Authorization': f'token {token}'}\n",
        "    params = {'q': query, 'per_page': per_page}\n",
        "    projets = []\n",
        "    page = 1\n",
        "\n",
        "    while True:\n",
        "        params['page'] = page\n",
        "        response = requests.get(url, headers=headers, params=params)\n",
        "        if response.status_code == 403:\n",
        "            print(\"Limite de taux dépassée. En attente pour réessayer...\")\n",
        "            time.sleep(200)  # Attendre 60 secondes avant de réessayer\n",
        "            continue\n",
        "        elif response.status_code != 200:\n",
        "            print(f\"Erreur lors de la récupération des projets : {response.status_code}\")\n",
        "            break\n",
        "        data = response.json()\n",
        "        projets.extend([item['repository']['full_name'] for item in data['items']])\n",
        "        if 'next' not in response.links:\n",
        "            break\n",
        "        page += 1\n",
        "\n",
        "    return projets\n",
        "\n",
        "\n",
        "def fetch_file_contents(repo_full_name, path, sha, token):\n",
        "    \"\"\"Fetch the contents of a file from GitHub for a specific commit SHA.\"\"\"\n",
        "    url = f\"https://api.github.com/repos/{repo_full_name}/git/trees/{sha}?recursive=1\"\n",
        "\n",
        "    headers = {'Authorization': f'token {token}', 'Accept': 'application/vnd.github.v3+json'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        tree = response.json().get('tree', [])\n",
        "        file_paths = [item['path'] for item in tree if item['type'] == 'blob']\n",
        "\n",
        "        total_lines = 0\n",
        "        for file_path in file_paths:\n",
        "            content_url = f\"https://api.github.com/repos/repo_full_name/contents/{file_path}?ref={sha}\"\n",
        "            content_response = requests.get(content_url, headers=headers)\n",
        "            if content_response.status_code == 200:\n",
        "                file_content = content_response.json().get('content', '')\n",
        "                file_lines = len(file_content.encode('utf-8').splitlines())\n",
        "                total_lines += file_lines\n",
        "\n",
        "        return total_lines\n",
        "    else:\n",
        "        logging.error(f\"Failed to fetch file contents, status code: {response.status_code}\")\n",
        "        return 0\n",
        "\n",
        "\n",
        "def get_repository_languages(repo_full_name, token):\n",
        "    \"\"\"Fetch the programming languages used in a given repository.\"\"\"\n",
        "    api_url = f'https://api.github.com/repos/{repo_full_name}/languages'\n",
        "    languages_data = get_request(api_url, token)  # This now correctly passes both arguments\n",
        "    if languages_data:\n",
        "        return ', '.join(languages_data.keys())\n",
        "    return \"Failed to fetch languages\"\n",
        "\n",
        "def calculate_sloc_via_github_api(repo_full_name, commit_sha, token):\n",
        "    url = f\"https://api.github.com/repos/{repo_full_name}/git/trees/{commit_sha}?recursive=true\"\n",
        "    headers = {'Authorization': f'token {token}'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        tree_data = response.json()\n",
        "        sloc = sum(1 for item in tree_data.get('tree', []) if item['type'] == 'blob')\n",
        "        return sloc\n",
        "    else:\n",
        "        logging.error(f\"Failed to fetch tree data, status code: {response.status_code}\")\n",
        "        return 0\n",
        "\n",
        "\n",
        "def remove_readonly_dir(target_dir):\n",
        "    # Check if the target directory exists\n",
        "    if not os.path.exists(target_dir):\n",
        "        print(f\"The directory {target_dir} does not exist.\")\n",
        "        return\n",
        "\n",
        "    # Walk through the directory\n",
        "    for root, dirs, files in os.walk(target_dir, topdown=False):\n",
        "        for name in files:\n",
        "            filepath = os.path.join(root, name)\n",
        "            # Change the file permission to writable\n",
        "            os.chmod(filepath, 0o666)\n",
        "        for name in dirs:\n",
        "            dirpath = os.path.join(root, name)\n",
        "            # Change the directory permission to writable\n",
        "            os.chmod(dirpath, 0o666)\n",
        "\n",
        "    # Once all permissions are changed, remove the directory\n",
        "    shutil.rmtree(target_dir)\n",
        "    print(f\"Successfully removed {target_dir}\")\n",
        "\n",
        "def get_pr_details(pr_url, token):\n",
        "    \"\"\"Fetch the pull request's title, description, and comments count.\"\"\"\n",
        "    pr_data = get_request(pr_url, token)\n",
        "    if pr_data:\n",
        "        title_words = len(pr_data.get('title', '').split())\n",
        "        body_words = len(pr_data.get('body', '') or ''.split())\n",
        "        comments_count = pr_data.get('comments', 0)\n",
        "        description_complexity = title_words + body_words\n",
        "        return description_complexity, comments_count\n",
        "    return 0, 0\n",
        "\n",
        "def get_commit_data(commit_sha, repo_full_name, token):\n",
        "    \"\"\"Fetch commit data for a given commit SHA, focusing on production and test code churn.\"\"\"\n",
        "    url = f\"https://api.github.com/repos/{repo_full_name}/commits/{commit_sha}\"\n",
        "    headers = {'Authorization': f'token {token}'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        logging.error(f\"Failed to fetch data for commit {commit_sha}, status code: {response.status_code}, URL: {url}, Response: {response.text if response.status_code != 422 else response.json()}\")\n",
        "        return None\n",
        "\n",
        "    commit_data = response.json()\n",
        "    if not commit_data:\n",
        "        return None\n",
        "\n",
        "    files = commit_data.get('files', [])\n",
        "    file_types = set([os.path.splitext(file.get('filename', ''))[1] for file in files])\n",
        "    src_churn, test_churn = calculate_churn(files)\n",
        "\n",
        "    added_files_count = sum(1 for file in files if file.get('status') == 'added_lines')\n",
        "    deleted_files_count = sum(1 for file in files if file.get('status') == 'deleted_lines')\n",
        "    modified_files_count = sum(1 for file in files if file.get('status') == 'modified')\n",
        "\n",
        "    return {\n",
        "        'added_lines': added_files_count,\n",
        "        'deleted_lines': deleted_files_count,\n",
        "        'modified_files': modified_files_count,\n",
        "        'tests_added': sum(file.get('additions', 0) for file in files if is_test_file(file.get('filename', ''))),\n",
        "        'tests_deleted': sum(file.get('deletions', 0) for file in files if is_test_file(file.get('filename', ''))),\n",
        "        'total_files': len(files),\n",
        "        'additions_files_commit': commit_data.get('stats', {}).get('additions', 0),\n",
        "        'deletions_files_commit': commit_data.get('stats', {}).get('deletions', 0),\n",
        "        'file_types': ', '.join(file_types),\n",
        "        'src_churn': src_churn,\n",
        "        'test_churn': test_churn,\n",
        "    }\n",
        "\n",
        "def calculate_churn(files):\n",
        "    \"\"\"Calculate source and test code churn from file data.\"\"\"\n",
        "    src_churn = test_churn = 0\n",
        "    for file in files:\n",
        "        added_lines = file.get('additions', 0)\n",
        "        deleted_lines = file.get('deletions', 0)\n",
        "        if is_test_file(file.get('filename', '')):\n",
        "            test_churn += added_lines + deleted_lines\n",
        "        else:\n",
        "            src_churn += added_lines + deleted_lines\n",
        "    return src_churn, test_churn\n",
        "\n",
        "def is_test_file(file_path):\n",
        "    \"\"\"Determine if the given file path corresponds to a testing file.\"\"\"\n",
        "    return 'test' in file_path.lower() or file_path.startswith('tests/')\n",
        "\n",
        "def is_specific_file_type(file_path, file_type='production'):\n",
        "    \"\"\"\n",
        "    Determine if a file path corresponds to a specific file type based on predefined criteria.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: The path of the file to evaluate.\n",
        "    - file_type: The type of file to check for ('production' or 'documentation').\n",
        "\n",
        "    Returns True if the file is considered of the specified type; False otherwise.\n",
        "    \"\"\"\n",
        "    if file_type == 'production':\n",
        "        extensions = ['.py', '.js', '.java', '.cpp', '.cs']\n",
        "        excluded_paths = ['tests', 'test']\n",
        "    elif file_type == 'documentation':\n",
        "        extensions = ['.md', '.rst', '.txt', '.docx']\n",
        "        excluded_paths = []\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "    file_extension = os.path.splitext(file_path)[1]\n",
        "    return file_extension in extensions and not any(excluded_path in file_path for excluded_path in excluded_paths)\n",
        "def get_pr_details(pr_url, token):\n",
        "    \"\"\"Fetch the pull request's title, description, and comments count.\"\"\"\n",
        "    pr_data = get_request(pr_url, token)\n",
        "    if pr_data:\n",
        "        title_words = len(pr_data.get('title', '').split())\n",
        "        body_words = len(pr_data.get('body', '') or ''.split())\n",
        "        comments_count = pr_data.get('comments', 0)\n",
        "        description_complexity = title_words + body_words\n",
        "        return description_complexity, comments_count\n",
        "    return 0, 0\n",
        "\n",
        "def get_builds_info(repo_full_name, token, output_csv):\n",
        "    repo_full_name = f\"{repo_full_name}\"\n",
        "    builds_info = []\n",
        "    languages = get_repository_languages(repo_full_name, token)\n",
        "\n",
        "    initial_url = f\"https://api.github.com/repos/{repo_full_name}/actions/runs?page=1&per_page=1\"\n",
        "    initial_response_data = get_request(initial_url, token)\n",
        "\n",
        "    if initial_response_data is None:\n",
        "        print(f\"Error: Failed to fetch initial data for {repo_full_name}.\")\n",
        "        return\n",
        "    total_builds = initial_response_data.get('total_count', 0)\n",
        "\n",
        "    # Continue only if there are 100 builds or more\n",
        "    if total_builds < 50:\n",
        "        print(f\"Repository {repo_full_name} has less than 100 builds. Skipping...\")\n",
        "        return\n",
        "    if total_builds > 5000:\n",
        "        print(f\"Repository {repo_full_name} has less than 5000 builds. Skipping...\")\n",
        "        return\n",
        "    page = 1\n",
        "    while True:\n",
        "        api_url = f\"https://api.github.com/repos/{repo_full_name}/actions/runs?page={page}&per_page=max\"\n",
        "\n",
        "        response_data = get_request(api_url, token)\n",
        "        if response_data is None:\n",
        "            print(\"Error: Failed to fetch data from GitHub API.\")\n",
        "            break\n",
        "        if page == 1:\n",
        "            total_builds = response_data.get('total_count', 0)  # Get the total number of builds only once\n",
        "\n",
        "        runs = response_data.get('workflow_runs', [])\n",
        "        if not runs:\n",
        "            break\n",
        "\n",
        "        for run in runs:\n",
        "            commit_sha = run['head_sha']\n",
        "            sloc = calculate_sloc_via_github_api(repo_full_name, commit_sha, token)\n",
        "            start_time = datetime.strptime(run['created_at'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "            end_time = datetime.strptime(run['updated_at'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "            duration = (end_time - start_time).total_seconds()\n",
        "            branch = run['head_branch']\n",
        "            commit_data = get_commit_data(commit_sha, repo_full_name, token)\n",
        "            tests_ran = True\n",
        "            tests_failed = run['conclusion'] != 'success'\n",
        "            gh_is_pr = len(run.get('pull_requests', [])) > 0\n",
        "            gh_description_complexity = 0\n",
        "            gh_num_pr_comments = 0  # Initialize this variable before it's possibly set\n",
        "\n",
        "            if gh_is_pr:\n",
        "                pr_url = run['pull_requests'][0]['url']\n",
        "                gh_description_complexity, gh_num_pr_comments = get_pr_details(pr_url, token)\n",
        "            if commit_data is None:  # Properly check if commit_data is None\n",
        "                print(f\"Error retrieving commit data for SHA {commit_sha}. Skipping this commit.\")\n",
        "                continue\n",
        "            build_info = {\n",
        "                'repo': repo_full_name,\n",
        "                'id_build': run['id'],\n",
        "                'branch': branch,\n",
        "                'status': run['status'],\n",
        "                'etat': run['conclusion'],\n",
        "                'created_at': run['created_at'],\n",
        "                'updated_at': run['updated_at'],\n",
        "                'build_duration': duration,\n",
        "                'sloc': sloc,\n",
        "                'added_lines': commit_data['added_lines'],\n",
        "                'deleted_lines': commit_data['deleted_lines'],\n",
        "                'modified_files': commit_data['modified_files'],\n",
        "                'tests_added': commit_data['tests_added'],\n",
        "                'tests_deleted': commit_data['tests_deleted'],\n",
        "                'total_files': commit_data['total_files'],\n",
        "                'tests_ran': tests_ran,\n",
        "                'tests_failed': tests_failed,\n",
        "                'additions_files_commit': commit_data['additions_files_commit'],\n",
        "                'deletions_files_commit': commit_data['deletions_files_commit'],\n",
        "                'src_churn': commit_data['src_churn'],\n",
        "                'test_churn': commit_data['test_churn'],\n",
        "                'file_types': commit_data['file_types'],\n",
        "                'languages': languages,\n",
        "                'gh_description_complexity': gh_description_complexity,\n",
        "                'gh_num_pr_comments': gh_num_pr_comments,\n",
        "                'gh_is_pr': gh_is_pr,\n",
        "                'total_builds': total_builds,\n",
        "            }\n",
        "            builds_info.append(build_info)\n",
        "\n",
        "        page += 1\n",
        "        if not runs:\n",
        "            break\n",
        "\n",
        "    with open(output_csv, mode='a', newline='', encoding='utf-8') as file:\n",
        "        fieldnames = ['repo', 'id_build', 'branch', 'status', 'etat', 'created_at', 'updated_at', 'build_duration', 'total_builds',\n",
        "                      'added_lines', 'deleted_lines', 'additions_files_commit', 'deletions_files_commit', 'modified_files', 'tests_added', 'tests_deleted',\n",
        "                      'total_files', 'tests_ran', 'tests_failed', 'src_churn', 'test_churn', 'file_types', 'languages', 'sloc', 'gh_description_complexity',\n",
        "                      'gh_num_pr_comments', 'gh_is_pr']\n",
        "\n",
        "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for build in builds_info:\n",
        "            writer.writerow(build)\n",
        "\n",
        "    print(f\"Build information saved to {output_csv}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    projets = chercher_projets_avec_build_yml(github_token)  # Dynamically fetch repositories\n",
        "    for projet in projets:\n",
        "        logging.info(f\"Processing repository: {projet}\")\n",
        "        get_builds_info(projet, github_token, output_csv)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}